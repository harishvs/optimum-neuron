{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Code Llama on AWS Trainium \n",
    "\n",
    "This tutorial will teach how to fine-tune open LLMs like [Llama 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) on AWS Trainium.  In our example, we are going to leverage Hugging Face Optimum Neuron, [Transformers](https://huggingface.co/docs/transformers/index)and datasets. \n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Setup AWS environment](#1-setup-aws-environment)\n",
    "2. [Load and process the dataset](#2-load-and-prepare-the-dataset)\n",
    "3. [Fine-tune Llama on AWS Trainium using the `NeuronTrainer`](#3-fine-tune-llama-on-aws-trainium-using-the-neurontrainer)\n",
    "4. [Evalaute and test fine-tuned Llama model](#4-evalaute-and-test-fine-tuned-llama-model)\n",
    "\n",
    "## Quick intro: AWS Trainium\n",
    "\n",
    "[AWS Trainium (Trn1)](https://aws.amazon.com/de/ec2/instance-types/trn1/) is a purpose-built EC2 for deep learning (DL) training workloads. Trainium is the successor of [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls) focused on high-performance training workloads. Trainium has been optimized for training natural language processing, computer vision, and recommender models used. The accelerator supports a wide range of data types, including FP32, TF32, BF16, FP16, UINT8, and configurable FP8. \n",
    "\n",
    "The biggest Trainium instance, the `trn1.32xlarge` comes with over 500GB of memory, making it easy to fine-tune ~10B parameter models on a single instance. Below you will find an overview of the available instance types. More details [here](https://aws.amazon.com/de/ec2/instance-types/trn1/#Product_details):\n",
    "\n",
    "| instance size | accelerators | accelerator memory | vCPU | CPU Memory | price per hour |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| trn1.2xlarge | 1 | 32 | 8 | 32 | \\$1.34 |\n",
    "| trn1.32xlarge | 16 | 512 | 128 | 512 | \\$21.50 |\n",
    "| trn1n.32xlarge (2x bandwidth) | 16 | 512 | 128 | 512 | \\$24.78 |\n",
    "\n",
    "---\n",
    "\n",
    "*Note: This tutorial was created on a trn1.32xlarge AWS EC2 Instance.* \n",
    "\n",
    "\n",
    "## 1. Setup AWS environment\n",
    "\n",
    "In this example, we will use the `trn1.32xlarge` instance on AWS with 16 Accelerator, including 32 Neuron Cores and the [Hugging Face Neuron Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2). The Hugging Face AMI comes with all important libraries, like Transformers, Datasets, Optimum and Neuron packages pre-installed this makes it super easy to get started, since there is no need for environment management.\n",
    "\n",
    "This blog post doesn’t cover how to create the instance in detail. You can check out my previous blog about [“Setting up AWS Trainium for Hugging Face Transformers”](https://www.philschmid.de/setup-aws-trainium), which includes a step-by-step guide on setting up the environment. \n",
    "\n",
    "Once the instance is up and running, we can ssh into it. But instead of developing inside a terminal we want to use a `Jupyter` environment, which we can use for preparing our dataset and launching the training. For this, we need to add a port for forwarding in the `ssh` command, which will tunnel our localhost traffic to the Trainium instance.\n",
    "\n",
    "```bash\n",
    "PUBLIC_DNS=\"\" # IP address, e.g. ec2-3-80-....\n",
    "KEY_PATH=\"\" # local path to key, e.g. ssh/trn.pem\n",
    "\n",
    "ssh -L 8080:localhost:8080 -i ${KEY_NAME}.pem ubuntu@$PUBLIC_DNS\n",
    "```\n",
    "\n",
    "Lets now pull the optimum repository with the [example notebook and scripts](https://github.com/huggingface/optimum-neuron/tree/main/notebooks/text-generation).\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/huggingface/optimum-neuron.git\n",
    "```\n",
    "\n",
    "Next we can change our directory to `notbooks/text-generation` and launch the `jupyter` environment.``\n",
    "\n",
    "\n",
    "```bash\n",
    "# change directory\n",
    "cd optimum-neuron/notebooks/text-generation\n",
    "# launch jupyter\n",
    "python -m notebook --allow-root --port=8080\n",
    "```\n",
    "\n",
    "You should see a familiar **`jupyter`** output with a URL to the notebook.\n",
    "\n",
    "**`http://localhost:8080/?token=8c1739aff1755bd7958c4cfccc8d08cb5da5234f61f129a9`**\n",
    "\n",
    "We can click on it, and a **`jupyter`** environment opens in our local browser. Open the notebook **`llama2-7b-fine-tuning.ipynb`** and lets get started.\n",
    "\n",
    "_Note: We are going to use the Jupyter environment only for preparing the dataset and then `torchrun` for launching our training script for  distributed training._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use official Llama 2 checkpoint you need to login into our hugging face account, which has access to the model, to use your token for accessing the gated repository. We can do this by running the following command:\n",
    "\n",
    "_Note: We also provide an ungated checkpoint._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze |grep neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token TOKEN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "We will use [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k) an open source dataset of instruction-following records on categories outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "\n",
    "To load the `dolly` dataset, we use the `load_dataset()` method from the 🤗 Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 6251\n",
      "test dataset size: 1147\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"dreamerdeo/finqa\", split=\"train\")\n",
    "test_dataset = load_dataset(\"dreamerdeo/finqa\", split=\"test\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(f\"test dataset size: {len(test_dataset)}\")\n",
    "# print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_finqa(sample):\n",
    "    post_text = f\"### PostText\\n{sample['post_text']}\"\n",
    "    pre_text = f\"### PreText\\n{sample['pre_text']}\"\n",
    "    question = f\"### Question\\n{sample['question']}\"\n",
    "    answer = f\"### Answer\\n{sample['answer']}\"\n",
    "    gold_evidence = f\"### GoldEvidence\\n{sample['gold_evidence']}\"\n",
    "    table = f\"### Table\\n{sample['table']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [post_text, pre_text, question,answer,gold_evidence, table] if i is not None])\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PostText\n",
      "['( i ) relates to unusual , non-recurring litigation matters .', '( ii ) includes certain retention costs and equity investment income , certain severance costs in 2009 and a gain related to the sale of the informacast software and equipment in 2009. .']\n",
      "\n",
      "### PreText\n",
      "['( 2 ) for purposes of calculating the ratio of earnings to fixed charges , earnings consist of earnings before income taxes minus income from equity investees plus fixed charges .', 'fixed charges consist of interest expense and the portion of rental expense we believe is representative of the interest component of rental expense .', '( a ) for the years ended december 31 , 2010 and 2009 , earnings available for fixed charges were inadequate to cover fixed charges by $ 37.0 million and $ 461.2 million , respectively .', '( 3 ) ebitda is defined as consolidated net income ( loss ) before interest expense , income tax expense ( benefit ) , depreciation , and amortization .', 'adjusted ebitda , which is a measure defined in our credit agreements , is calculated by adjusting ebitda for certain items of income and expense including ( but not limited to ) the following : ( a ) non-cash equity-based compensation ; ( b ) goodwill impairment charges ; ( c ) sponsor fees ; ( d ) certain consulting fees ; ( e ) debt-related legal and accounting costs ; ( f ) equity investment income and losses ; ( g ) certain severance and retention costs ; ( h ) gains and losses from the early extinguishment of debt ; ( i ) gains and losses from asset dispositions outside the ordinary course of business ; and ( j ) non-recurring , extraordinary or unusual gains or losses or expenses .', 'we have included a reconciliation of ebitda and adjusted ebitda in the table below .', 'both ebitda and adjusted ebitda are considered non-gaap financial measures .', 'generally , a non-gaap financial measure is a numerical measure of a company 2019s performance , financial position or cash flows that either excludes or includes amounts that are not normally included or excluded in the most directly comparable measure calculated and presented in accordance with gaap .', 'non-gaap measures used by the company may differ from similar measures used by other companies , even when similar terms are used to identify such measures .', 'we believe that ebitda and adjusted ebitda provide helpful information with respect to our operating performance and cash flows including our ability to meet our future debt service , capital expenditures and working capital requirements .', 'adjusted ebitda also provides helpful information as it is the primary measure used in certain financial covenants contained in our credit agreements .', 'the following unaudited table sets forth reconciliations of net income ( loss ) to ebitda and ebitda to adjusted ebitda for the periods presented: .']\n",
      "\n",
      "### Question\n",
      "3 net income ( loss ) $ 132.8 \\\\n5 income tax expense ( benefit ) 62.7\n",
      "\n",
      "### Answer\n",
      "32.1%\n",
      "\n",
      "### GoldEvidence\n",
      "['( in millions ) the net income ( loss ) of years ended december 31 , 2013 is $ 132.8 ; the net income ( loss ) of years ended december 31 , 2012 is $ 119.0 ; the net income ( loss ) of years ended december 31 , 2011 is $ 17.1 ; the net income ( loss ) of years ended december 31 , 2010 is $ -29.2 ( 29.2 ) ; the net income ( loss ) of years ended december 31 , 2009 is $ -373.4 ( 373.4 ) ;', '( in millions ) the income tax expense ( benefit ) of years ended december 31 , 2013 is 62.7 ; the income tax expense ( benefit ) of years ended december 31 , 2012 is 67.1 ; the income tax expense ( benefit ) of years ended december 31 , 2011 is 11.2 ; the income tax expense ( benefit ) of years ended december 31 , 2010 is -7.8 ( 7.8 ) ; the income tax expense ( benefit ) of years ended december 31 , 2009 is -87.8 ( 87.8 ) ;']\n",
      "\n",
      "### Table\n",
      "[['( in millions )', 'years ended december 31 , 2013', 'years ended december 31 , 2012', 'years ended december 31 , 2011', 'years ended december 31 , 2010', 'years ended december 31 , 2009'], ['net income ( loss )', '$ 132.8', '$ 119.0', '$ 17.1', '$ -29.2 ( 29.2 )', '$ -373.4 ( 373.4 )'], ['depreciation and amortization', '208.2', '210.2', '204.9', '209.4', '218.2'], ['income tax expense ( benefit )', '62.7', '67.1', '11.2', '-7.8 ( 7.8 )', '-87.8 ( 87.8 )'], ['interest expense net', '250.1', '307.4', '324.2', '391.9', '431.7'], ['ebitda', '653.8', '703.7', '557.4', '564.3', '188.7'], ['non-cash equity-based compensation', '8.6', '22.1', '19.5', '11.5', '15.9'], ['sponsor fees', '2.5', '5.0', '5.0', '5.0', '5.0'], ['consulting and debt-related professional fees', '0.1', '0.6', '5.1', '15.1', '14.1'], ['goodwill impairment', '2014', '2014', '2014', '2014', '241.8'], ['net loss ( gain ) on extinguishments of long-term debt', '64.0', '17.2', '118.9', '-2.0 ( 2.0 )', '2014'], ['litigation net ( i )', '-4.1 ( 4.1 )', '4.3', '2014', '2014', '2014'], ['ipo- and secondary-offering related expenses', '75.0', '2014', '2014', '2014', '2014'], ['other adjustments ( ii )', '8.6', '13.7', '11.4', '7.9', '-0.1 ( 0.1 )'], ['adjusted ebitda', '$ 808.5', '$ 766.6', '$ 717.3', '$ 601.8', '$ 465.4']]\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_finqa(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training. This means that we are stacking multiple samples to one sequence and split them with an EOS Token. This makes the training more efficient. Packing/stacking samples can be done during training or before. We will do it before training to save time. We created a utility method [pack_dataset](./scripts/utils/pack_dataset.py) that takes a dataset and a packing function and returns a packed dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"codellama/CodeLlama-7b-hf\"\n",
    "# model_id = \"codellama/CodeLlama-13b-hf\"\n",
    "# model_id = \"codellama/CodeLlama-34b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pack/stack our dataset we need to first tokenize it and then we can pack it with the `pack_dataset` method. To prepare our dataset we will now: \n",
    "1. Format our samples using the template method and add an EOS token at the end of each sample\n",
    "2. Tokenize our dataset to convert it from text to tokens\n",
    "3. Pack our dataset to 2048 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PostText\n",
      "['as part of the purchase price allocation , all intangible assets that were a part of the acquisition were identified and valued .', 'it was determined that only customer relationship , trade name and developed technology had separately identifiable values .', 'the fair value of these intangible assets was determined through the application of the income approach .', 'customer relationship represented a large customer base that was expected to purchase the disposable mammopad product on a regular basis .', 'trade name represented the biolucent product name that the company intended to continue to use .', 'developed technology represented currently marketable purchased products that the company continues to sell as well as utilize to enhance and incorporate into the company 2019s existing products .', 'the deferred income tax liability relates to the tax effect of acquired identifiable intangible assets and fair value adjustments to acquired inventory , as such amounts are not deductible for tax purposes , partially offset by acquired net operating loss carryforwards of approximately $ 2400 .', '4 .', 'sale of gestiva on january 16 , 2008 , the company entered into a definitive agreement pursuant to which it agreed to sell full u.s .', 'and world-wide rights to gestiva to k-v pharmaceutical company upon approval of the pending gestiva new drug application ( the 201cgestiva nda 201d ) by the fda for a purchase price of $ 82000 .', 'the company received $ 9500 of the purchase price in fiscal 2008 , and the balance is due upon final approval of the gestiva nda by the fda on or before february 19 , 2010 and the production of a quantity of gestiva suitable to enable the commercial launch of the product .', 'either party has the right to terminate the agreement if fda approval is not obtained by february 19 , 2010 .', 'the company agreed to continue its efforts to obtain fda approval of the nda for gestiva as part of this arrangement .', 'all costs incurred in these efforts will be reimbursed by k-v pharmaceutical and are being recorded as a credit against research and development expenses .', 'during fiscal 2009 and 2008 , these reimbursed costs were not material .', 'the company recorded the $ 9500 as a deferred gain within current liabilities in the consolidated balance sheet .', 'the company expects that the gain will be recognized upon the closing of the transaction following final fda approval of the gestiva nda or if the agreement is terminated .', 'the company cannot assure that it will be able to obtain the requisite fda approval , that the transaction will be completed or that it will receive the balance of the purchase price .', 'moreover , if k-v pharmaceutical terminates the agreement as a result of a breach by the company of a material representation , warranty , covenant or agreement , the company will be required to return the funds previously received as well as expenses reimbursed by k-v .', 'source : hologic inc , 10-k , november 24 , 2009 powered by morningstar ae document research 2120 the information contained herein may not be copied , adapted or distributed and is not warranted to be accurate , complete or timely .', 'the user assumes all risks for any damages or losses arising from any use of this information , except to the extent such damages or losses cannot be limited or excluded by applicable law .', 'past financial performance is no guarantee of future results. .']\n",
      "\n",
      "### PreText\n",
      "['table of contents hologic , inc .', 'notes to consolidated financial statements ( continued ) ( in thousands , except per share data ) the acquisition also provides for up to two annual earn-out payments not to exceed $ 15000 in the aggregate based on biolucent 2019s achievement of certain revenue targets .', 'the company considered the provision of eitf 95-8 , and concluded that this contingent consideration represents additional purchase price .', 'as a result , goodwill will be increased by the amount of the additional consideration , if any , as it is earned .', 'as of september 26 , 2009 , the company has not recorded any amounts for these potential earn-outs .', 'the allocation of the purchase price was based upon estimates of the fair value of assets acquired and liabilities assumed as of september 18 , 2007 .', 'the components and allocation of the purchase price consisted of the following approximate amounts: .']\n",
      "\n",
      "### Question\n",
      "what portion of the final purchase price is related to goodwill?\n",
      "\n",
      "### Answer\n",
      "65.3%\n",
      "\n",
      "### GoldEvidence\n",
      "['net tangible assets acquired as of september 18 2007 the goodwill of $ 2800 is 47800 ;', 'net tangible assets acquired as of september 18 2007 the final purchase price of $ 2800 is $ 73200 ;']\n",
      "\n",
      "### Table\n",
      "[['net tangible assets acquired as of september 18 2007', '$ 2800'], ['developed technology and know how', '12300'], ['customer relationship', '17000'], ['trade name', '2800'], ['deferred income tax liabilities net', '-9500 ( 9500 )'], ['goodwill', '47800'], ['final purchase price', '$ 73200']]</s>\n",
      "Chunking dataset into chunks of 2048 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9236ac7d72f42d78426b32d5bb99cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 4099\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "# add utils method to path for loading dataset\n",
    "import sys\n",
    "sys.path.append(\"./scripts/utils\") # make sure you change this to the correct path \n",
    "from pack_dataset import pack_dataset\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_finqa(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# tokenize dataset\n",
    "dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")\n",
    "\n",
    "# chunk dataset\n",
    "lm_dataset = pack_dataset(dataset, chunk_length=2048) # We use 2048 as the maximum length for packing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going save it to disk. You could also save it to S3 or the Hugging Face Hub for later use. \n",
    "\n",
    "_Note: Packing and preprocessing your dataset can be run outside of the Trainium instance._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285e594ec9dc46d79cbdb3551e7f1c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4099 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save train_dataset to disk\n",
    "dataset_path = \"tokenized_finqa\"\n",
    "lm_dataset.save_to_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf finqa_codellama"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Llama on AWS Trainium using the `NeuronTrainer`\n",
    "\n",
    "Normally you would use the **[Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer)** and **[TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments)** to fine-tune PyTorch-based transformer models. \n",
    "\n",
    "But together with AWS, we have developed a `NeuronTrainer` to improve performance, robustness, and safety when training on Trainium instances. The `NeuronTrainer` is part of the `optimum-neuron` library and can be used as a 1-to-1 replacement for the `Trainer`.\n",
    "\n",
    "When it comes to distributed training on AWS Trainium there is a few things we need to take care of. Since Llama is a big model it might not fit on a single accelerator, thats why we added support for different distributed training strategies to the `NeuronTrainer` including: \n",
    "* [ZeRO-1](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/tutorials/training/zero1_gpt2.html): shards the optimizer state over multiple devices.\n",
    "* [Tensor Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html): shards the model parameters along a given dimension on multiple devices, defined with `tensor_parallel_size`\n",
    "* [Sequence parallelism](https://arxiv.org/pdf/2205.05198.pdf) shards the activations on the sequence axis outside of the tensor parallel regions. It is useful because it saves memory by sharding the activations.\n",
    "* [Pipeline Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/pipeline_parallelism_overview.html): _coming soon_\n",
    "\n",
    "\n",
    "We prepared a [run_clm.py](./scripts/run_clm.py), which implements those distributed training strategies for you already. If you want to more about the details you can take a look at the [documentation](https://huggingface.co/docs/optimum-neuron/guides/distributed_training). When training models on AWS Accelerators we first need to compile our model with our training arguments. \n",
    "\n",
    "To overcome this we added a [model cache](https://huggingface.co/docs/optimum-neuron/guides/cache_system), which allows us to use precompiled models and configuration from Hugging Face Hub to skip the compilation step. But every change in the config, will lead to a new compilation, which could result in some cache misses. \n",
    "\n",
    "_Note: If your configuration is not cached please open an issue on [Github](https://github.com/huggingface/optimum-neuron/issues), we are happy to include it._\n",
    "\n",
    "We pre-compiled the config for our training already meaning you can either skip the cell below or rerun it will only take a few minutes since it reuses the cached configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Compiling without a cache can take ~40 minutes. It will also create dummy files in the `dolly_llama_sharded` during compilation you we have to remove them afterwards. We also need to add `MALLOC_ARENA_MAX=64` to limit the CPU allocation to avoid potential crashes, don't remove it for now._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-15 00:17:10.000057:  1173301  INFO ||NEURON_PARALLEL_COMPILE||: Removing existing workdir /tmp/ubuntu/parallel_compile_workdir\n",
      "2024-02-15 00:17:10.000057:  1173301  INFO ||NEURON_PARALLEL_COMPILE||: Running trial run (add option to terminate trial run early; also ignore trial run's generated outputs, i.e. loss, checkpoints)\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "Please use the TrainiumTrainer from optimum[neuron] instead of the Transformers library to perform training on AWS Trainium instances. More information here: https://github.com/huggingface/optimum-neuron\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "*************************loaded data\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "got tokenizer\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "> initializing tensor model parallel with size 8\n",
      "> initializing pipeline model parallel with size 1\n",
      "> initializing data parallel with size 4\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "********************got model\n",
      "No Neuron cache name is saved locally. This means that only the official Neuron cache will be used. You can create a Neuron cache repo by running the following command: `optimum-cli neuron cache create`. If the Neuron cache already exists you can set it by running the following command: `optimum-cli neuron cache set -n [name]`.\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER***************got TRAINER\n",
      "\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "You do not have write access to aws-neuron/optimum-neuron-cache so you will not be able to push any cached compilation files. Please log in and/or use a custom Neuron cache.\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n",
      "***************got TRAINER\n"
     ]
    }
   ],
   "source": [
    "# precompilation command \n",
    "!MALLOC_ARENA_MAX=64 neuron_parallel_compile torchrun --nproc_per_node=32 scripts/run_clm.py \\\n",
    " --model_id {model_id} \\\n",
    " --dataset_path {dataset_path} \\\n",
    " --bf16 True \\\n",
    " --learning_rate 5e-5 \\\n",
    " --output_dir finqa_codellama \\\n",
    " --overwrite_output_dir True \\\n",
    " --per_device_train_batch_size 1 \\\n",
    " --gradient_checkpointing True \\\n",
    " --tensor_parallel_size 8 \\\n",
    " --max_steps 10 \\\n",
    " --logging_steps 10 \\\n",
    " --gradient_accumulation_steps 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dummy artifacts which are created by the precompilation command\n",
    "!rm -rf finqa_codellama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the compilation is done we can start our training with a similar command, we just need to remove the `neuron_parallel_compile`. We will use `torchrun` to launch our training script. `torchrun` is a tool that automatically distributes a PyTorch model across multiple accelerators. We can pass the number of accelerators as `nproc_per_node` arguments alongside our hyperparameters. \n",
    "The difference to the compilation command is that we changed from `max_steps=10` to `num_train_epochs=3`.\n",
    "\n",
    "Launch the training, with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!MALLOC_ARENA_MAX=64 torchrun --nproc_per_node=32 scripts/run_clm.py \\\n",
    " --model_id {model_id} \\\n",
    " --dataset_path {dataset_path} \\\n",
    " --bf16 True \\\n",
    " --learning_rate 5e-5 \\\n",
    " --output_dir finqa_codellama \\\n",
    " --overwrite_output_dir True \\\n",
    " --skip_cache_push True \\\n",
    " --per_device_train_batch_size 1 \\\n",
    " --gradient_checkpointing True \\\n",
    " --tensor_parallel_size 8 \\\n",
    " --num_train_epochs 3 \\\n",
    " --logging_steps 10 \\\n",
    " --gradient_accumulation_steps 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it, we successfully trained Llama 7B on AWS Trainium. The training took for 3 epochs on dolly (15k samples) took 43:24 minutes where the raw training time was only 31:46 minutes. This leads to a cost of ~$15.5 for the e2e training on the trn1.32xlarge instance. Not Bad! \n",
    "\n",
    "But before we can share and test our model we need to consolidate our model. Since we used Tensor Parallelism during training, we need to consolidate the model weights before we can use it. Tensor Parallelism shards the model weights accross different workers, only sharded checkpoints will be saved during training.\n",
    "\n",
    "The Optimum CLI provides a way of doing that very easily via the `optimum neuron consolidate`` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli neuron consolidate finqa_codellama/tensor_parallel_shards finqa_codellama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets remove our \"sharded\" checkpoints as we have consolidated them already to safetensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf finqa_codellama/tensor_parallel_shards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evalaute and test fine-tuned Llama model\n",
    "\n",
    "Similar to training to be able to run inferece on AWS Trainium or AWS Inferentia2 we need to compile our model for the correct use. We will use our Trainium instance for the inference test, but we recommend customer to switch to Inferentia2 for inference. \n",
    "\n",
    "Optimum Neuron implements similar to Transformers AutoModel classes for easy inference use. We will use  the `NeuronModelForCausalLM` class to load our vanilla transformers checkpoint and convert it to neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "compiler_args = {\"num_cores\": 2, \"auto_cast_type\": 'fp16'}\n",
    "input_shapes = {\"batch_size\": 1, \"sequence_length\": 2048}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"finqa_codellama\")\n",
    "model = NeuronModelForCausalLM.from_pretrained(\n",
    "        \"finqa_codellama\",\n",
    "        export=True,\n",
    "        **compiler_args,\n",
    "        **input_shapes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Inference compilation can take ~25minutes. Luckily, you need to only run this onces. Since you can save the model afterwards. If you are going to run on Inferentia2 you need to recompile again. The compilation is parameter and hardware specific._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENT IN if you want to save the compiled model\n",
    "# model.save_pretrained(\"compiled_finqa_codellama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test inference, but have to make sure we format our input to our prompt format we used for fine-tuning. Therefore we created a helper method, which accepts a `dict` with our `instruction` and optionally a `context`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_finqa_infernece(sample):\n",
    "    post_text = f\"### PostText\\n{sample['post_text']}\"\n",
    "    pre_text = f\"### PreText\\n{sample['pre_text']}\"\n",
    "    table = f\"### Table\\n{sample['table']}\"    \n",
    "    question = f\"### Question\\n{sample['question']}\"    \n",
    "    answer = f\"### Answer\\n\"\n",
    "    gold_evidence = f\"### GoldEvidence\\n\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [pre_text,post_text,table, question,answer,gold_evidence] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate(sample): \n",
    "    prompt = format_finqa_infernece(sample)\n",
    "    print(\"prompt\",prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs,\n",
    "                         max_new_tokens=512,\n",
    "                         do_sample=True,\n",
    "                         temperature=0.9,\n",
    "                         top_k=50,\n",
    "                         top_p=0.9)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=False)[len(prompt):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test inference. First we test without a context.\n",
    "\n",
    "_Note: Inference is not expected to be super fast on AWS Trainium using 2 cores. For Inference we recommend using Inferentia2._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_element = test_dataset[randrange(len(test_dataset))]\n",
    "prompt = {\n",
    "  \"pre_text\": test_data_element['pre_text'],\n",
    "  \"post_text\": test_data_element['post_text'],\n",
    "  \"question\": test_data_element['question'],\n",
    "  \"table\": test_data_element['table'],\n",
    "    \"gold_evidence\": test_data_element['gold_evidence']\n",
    "}\n",
    "res = generate(prompt)\n",
    "print(\"================================\")\n",
    "print(\"Actual Answer\",res)\n",
    "print(\"Expected Answer\",test_data_element['answer'])\n",
    "print(\"================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AWS stands for Amazon Web Services. AWS is a suite of remote computing services offered by Amazon. The most widely used of these include Amazon Elastic Compute Cloud (Amazon EC2), which provides resizable compute capacity in the cloud; Amazon Simple Storage Service (Amazon S3), which is an object storage service; and Amazon Elastic Block Store (Amazon EBS), which is designed to provide high performance, durable block storage volumes for use with AWS instances. AWS also provides other services, such as AWS Identity and Access Management (IAM), a service that enables organizations to control access to their AWS resources, and AWS Key Management Service (AWS KMS), which helps customers create and control the use of encryption keys.</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks correct. Now, lets add some context, e.g. as you would do for RAG applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, our model also correctly uses the provided context. We are done. Congrats on fine-tuning Code Llama on AWS Trainium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
