{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Code Llama on AWS Trainium \n",
    "\n",
    "This tutorial will teach how to fine-tune open LLMs like [Llama 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) on AWS Trainium.  In our example, we are going to leverage Hugging Face Optimum Neuron, [Transformers](https://huggingface.co/docs/transformers/index)and datasets. \n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. [Setup AWS environment](#1-setup-aws-environment)\n",
    "2. [Load and process the dataset](#2-load-and-prepare-the-dataset)\n",
    "3. [Fine-tune Llama on AWS Trainium using the `NeuronTrainer`](#3-fine-tune-llama-on-aws-trainium-using-the-neurontrainer)\n",
    "4. [Evalaute and test fine-tuned Llama model](#4-evalaute-and-test-fine-tuned-llama-model)\n",
    "\n",
    "## Quick intro: AWS Trainium\n",
    "\n",
    "[AWS Trainium (Trn1)](https://aws.amazon.com/de/ec2/instance-types/trn1/) is a purpose-built EC2 for deep learning (DL) training workloads. Trainium is the successor of [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls) focused on high-performance training workloads. Trainium has been optimized for training natural language processing, computer vision, and recommender models used. The accelerator supports a wide range of data types, including FP32, TF32, BF16, FP16, UINT8, and configurable FP8. \n",
    "\n",
    "The biggest Trainium instance, the `trn1.32xlarge` comes with over 500GB of memory, making it easy to fine-tune ~10B parameter models on a single instance. Below you will find an overview of the available instance types. More details [here](https://aws.amazon.com/de/ec2/instance-types/trn1/#Product_details):\n",
    "\n",
    "| instance size | accelerators | accelerator memory | vCPU | CPU Memory | price per hour |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| trn1.2xlarge | 1 | 32 | 8 | 32 | \\$1.34 |\n",
    "| trn1.32xlarge | 16 | 512 | 128 | 512 | \\$21.50 |\n",
    "| trn1n.32xlarge (2x bandwidth) | 16 | 512 | 128 | 512 | \\$24.78 |\n",
    "\n",
    "---\n",
    "\n",
    "*Note: This tutorial was created on a trn1.32xlarge AWS EC2 Instance.* \n",
    "\n",
    "\n",
    "## 1. Setup AWS environment\n",
    "\n",
    "In this example, we will use the `trn1.32xlarge` instance on AWS with 16 Accelerator, including 32 Neuron Cores and the [Hugging Face Neuron Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2). The Hugging Face AMI comes with all important libraries, like Transformers, Datasets, Optimum and Neuron packages pre-installed this makes it super easy to get started, since there is no need for environment management.\n",
    "\n",
    "This blog post doesnâ€™t cover how to create the instance in detail. You can check out my previous blog about [â€œSetting up AWS Trainium for Hugging Face Transformersâ€](https://www.philschmid.de/setup-aws-trainium), which includes a step-by-step guide on setting up the environment. \n",
    "\n",
    "Once the instance is up and running, we can ssh into it. But instead of developing inside a terminal we want to use a `Jupyter` environment, which we can use for preparing our dataset and launching the training. For this, we need to add a port for forwarding in theÂ `ssh` command, which will tunnel our localhost traffic to the Trainium instance.\n",
    "\n",
    "```bash\n",
    "PUBLIC_DNS=\"\" # IP address, e.g. ec2-3-80-....\n",
    "KEY_PATH=\"\" # local path to key, e.g. ssh/trn.pem\n",
    "\n",
    "ssh -L 8080:localhost:8080 -i ${KEY_NAME}.pem ubuntu@$PUBLIC_DNS\n",
    "```\n",
    "\n",
    "Lets now pull the optimum repository with the [example notebook and scripts](https://github.com/huggingface/optimum-neuron/tree/main/notebooks/text-generation).\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/huggingface/optimum-neuron.git\n",
    "```\n",
    "\n",
    "Next we can change our directory to `notbooks/text-generation` and launch the `jupyter` environment.``\n",
    "\n",
    "\n",
    "```bash\n",
    "# change directory\n",
    "cd optimum-neuron/notebooks/text-generation\n",
    "# launch jupyter\n",
    "python -m notebook --allow-root --port=8080\n",
    "```\n",
    "\n",
    "You should see a familiarÂ **`jupyter`**Â output with a URL to the notebook.\n",
    "\n",
    "**`http://localhost:8080/?token=8c1739aff1755bd7958c4cfccc8d08cb5da5234f61f129a9`**\n",
    "\n",
    "We can click on it, and aÂ **`jupyter`**Â environment opens in our local browser. Open the notebookÂ **`llama2-7b-fine-tuning.ipynb`**Â and lets get started.\n",
    "\n",
    "_Note: We are going to use the Jupyter environment only for preparing the dataset and then `torchrun` for launching our training script for  distributed training._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use official Llama 2 checkpoint you need to login into our hugging face account, which has access to the model, to use your token for accessing the gated repository. We can do this by running the following command:\n",
    "\n",
    "_Note: We also provide an ungated checkpoint._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws-neuronx-runtime-discovery==2.9\n",
      "libneuronxla==0.5.669\n",
      "neuronx-cc==2.12.68.0+4480452af\n",
      "neuronx-distributed==0.6.0\n",
      "neuronx-hwm==2.12.0.0+422c9037c\n",
      "optimum-neuron==0.0.17\n",
      "tensorboard-plugin-neuronx==2.6.1.0\n",
      "torch-neuronx==1.13.1.1.13.0\n",
      "torch-xla==1.13.1+torchneurond\n",
      "transformers-neuronx==0.9.474\n"
     ]
    }
   ],
   "source": [
    "!pip freeze |grep neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: optimum-neuron in /home/ubuntu/.local/lib/python3.8/site-packages (0.0.18)\n",
      "Requirement already satisfied, skipping upgrade: protobuf<4 in /usr/local/lib/python3.8/dist-packages (from optimum-neuron) (3.20.2)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub>=0.20.1 in /usr/local/lib/python3.8/dist-packages (from optimum-neuron) (0.20.3)\n",
      "Requirement already satisfied, skipping upgrade: accelerate==0.23.0 in /usr/local/lib/python3.8/dist-packages (from optimum-neuron) (0.23.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy<=1.25.2,>=1.22.2 in /usr/local/lib/python3.8/dist-packages (from optimum-neuron) (1.24.4)\n",
      "Requirement already satisfied, skipping upgrade: optimum>=1.16.2 in /usr/local/lib/python3.8/dist-packages (from optimum-neuron) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: transformers==4.36.2 in /usr/local/lib/python3.8/dist-packages (from optimum-neuron) (4.36.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.20.1->optimum-neuron) (2.31.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.20.1->optimum-neuron) (4.9.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.20.1->optimum-neuron) (4.66.1)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.1->optimum-neuron) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.20.1->optimum-neuron) (23.2)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.1->optimum-neuron) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.20.1->optimum-neuron) (2023.10.0)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from accelerate==0.23.0->optimum-neuron) (1.13.1)\n",
      "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate==0.23.0->optimum-neuron) (5.9.8)\n",
      "Requirement already satisfied, skipping upgrade: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum>=1.16.2->optimum-neuron) (15.0.1)\n",
      "Requirement already satisfied, skipping upgrade: sympy in /usr/local/lib/python3.8/dist-packages (from optimum>=1.16.2->optimum-neuron) (1.12)\n",
      "Requirement already satisfied, skipping upgrade: datasets in /usr/local/lib/python3.8/dist-packages (from optimum>=1.16.2->optimum-neuron) (2.16.1)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.8/dist-packages (from transformers==4.36.2->optimum-neuron) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.36.2->optimum-neuron) (2023.12.25)\n",
      "Requirement already satisfied, skipping upgrade: safetensors>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.36.2->optimum-neuron) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.1->optimum-neuron) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.20.1->optimum-neuron) (3.3.2)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.1->optimum-neuron) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.1->optimum-neuron) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate==0.23.0->optimum-neuron) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate==0.23.0->optimum-neuron) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate==0.23.0->optimum-neuron) (11.10.3.66)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate==0.23.0->optimum-neuron) (8.5.0.96)\n",
      "Requirement already satisfied, skipping upgrade: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum>=1.16.2->optimum-neuron) (10.0)\n",
      "Requirement already satisfied, skipping upgrade: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum>=1.16.2->optimum-neuron) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.16.2->optimum-neuron) (0.3.7)\n",
      "Requirement already satisfied, skipping upgrade: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.16.2->optimum-neuron) (3.4.1)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.16.2->optimum-neuron) (3.9.1)\n",
      "Requirement already satisfied, skipping upgrade: pyarrow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.16.2->optimum-neuron) (15.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.16.2->optimum-neuron) (2.0.3)\n",
      "Requirement already satisfied, skipping upgrade: pyarrow-hotfix in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.16.2->optimum-neuron) (0.6)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum>=1.16.2->optimum-neuron) (0.70.15)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/lib/python3/dist-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch>=1.10.0->accelerate==0.23.0->optimum-neuron) (45.2.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /usr/lib/python3/dist-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\"->torch>=1.10.0->accelerate==0.23.0->optimum-neuron) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<5.0,>=4.0; python_version < \"3.11\" in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum>=1.16.2->optimum-neuron) (4.0.3)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum>=1.16.2->optimum-neuron) (1.9.4)\n",
      "Requirement already satisfied, skipping upgrade: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum>=1.16.2->optimum-neuron) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum>=1.16.2->optimum-neuron) (23.1.0)\n",
      "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum>=1.16.2->optimum-neuron) (6.0.4)\n",
      "Requirement already satisfied, skipping upgrade: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum>=1.16.2->optimum-neuron) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum>=1.16.2->optimum-neuron) (2023.3.post1)\n",
      "Requirement already satisfied, skipping upgrade: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum>=1.16.2->optimum-neuron) (2023.4)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum>=1.16.2->optimum-neuron) (2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum>=1.16.2->optimum-neuron) (1.14.0)\n",
      "Requirement already up-to-date: aws-neuronx-runtime-discovery in /usr/local/lib/python3.8/dist-packages (2.9)\n",
      "Collecting libneuronxla\n",
      "  Downloading libneuronxla-1.0-py3-none-any.whl (1.4 kB)\n",
      "\u001b[31mERROR: torch-neuronx 1.13.1.1.13.0 has requirement libneuronxla==0.5.669, but you'll have libneuronxla 1.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: libneuronxla\n",
      "Successfully installed libneuronxla-1.0\n",
      "Requirement already up-to-date: neuronx-cc in /usr/local/lib/python3.8/dist-packages (2.12.68.0+4480452af)\n",
      "Requirement already satisfied, skipping upgrade: islpy<=2023.1,>2021.1 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc) (2023.1)\n",
      "Requirement already satisfied, skipping upgrade: neuronx-hwm==2.12.0.0 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc) (2.12.0.0+422c9037c)\n",
      "Requirement already satisfied, skipping upgrade: ec2-metadata<=2.10.0 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: networkx<=2.6.3 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc) (2.6.3)\n",
      "Requirement already satisfied, skipping upgrade: pgzip>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc) (0.3.5)\n",
      "Collecting protobuf<3.20\n",
      "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: psutil>=5.6.7 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc) (5.9.8)\n",
      "Requirement already satisfied, skipping upgrade: requests-unixsocket>=0.1.5 in /usr/lib/python3/dist-packages (from neuronx-cc) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy<=1.11.2 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc) (1.10.1)\n",
      "Requirement already satisfied, skipping upgrade: python-daemon>=2.2.4 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy<=1.25.2,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from neuronx-cc) (1.24.4)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.8/dist-packages (from ec2-metadata<=2.10.0->neuronx-cc) (2.31.0)\n",
      "Requirement already satisfied, skipping upgrade: lockfile>=0.10 in /usr/local/lib/python3.8/dist-packages (from python-daemon>=2.2.4->neuronx-cc) (0.12.2)\n",
      "Collecting setuptools>=62.4.0\n",
      "  Downloading setuptools-69.1.0-py3-none-any.whl (819 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 819 kB 179.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: docutils in /usr/local/lib/python3.8/dist-packages (from python-daemon>=2.2.4->neuronx-cc) (0.20.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->ec2-metadata<=2.10.0->neuronx-cc) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->ec2-metadata<=2.10.0->neuronx-cc) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->ec2-metadata<=2.10.0->neuronx-cc) (3.3.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->ec2-metadata<=2.10.0->neuronx-cc) (2019.11.28)\n",
      "\u001b[31mERROR: launchpadlib 1.10.13 requires testresources, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: torch-neuronx 1.13.1.1.13.0 has requirement libneuronxla==0.5.669, but you'll have libneuronxla 1.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: protobuf, setuptools\n",
      "Successfully installed protobuf-3.19.6 setuptools-69.1.0\n",
      "/usr/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import load_entry_point\n",
      "Requirement already up-to-date: neuronx-distributed in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: torch-neuronx in /usr/local/lib/python3.8/dist-packages (from neuronx-distributed) (1.13.1.1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: torch-xla in /usr/local/lib/python3.8/dist-packages (from neuronx-distributed) (1.13.1+torchneurond)\n",
      "Requirement already satisfied, skipping upgrade: torch==1.13.* in /usr/local/lib/python3.8/dist-packages (from torch-neuronx->neuronx-distributed) (1.13.1)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement libneuronxla==0.5.669 (from torch-neuronx->neuronx-distributed) (from versions: 1.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for libneuronxla==0.5.669 (from torch-neuronx->neuronx-distributed)\u001b[0m\n",
      "/usr/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import load_entry_point\n",
      "Requirement already up-to-date: neuronx-hwm in /usr/local/lib/python3.8/dist-packages (2.12.0.0+422c9037c)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from neuronx-hwm) (1.24.4)\n",
      "/usr/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import load_entry_point\n",
      "Requirement already up-to-date: torch-neuronx in /usr/local/lib/python3.8/dist-packages (1.13.1.1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: torch==1.13.* in /usr/local/lib/python3.8/dist-packages (from torch-neuronx) (1.13.1)\n",
      "Requirement already satisfied, skipping upgrade: torch-xla==1.13.1+torchneurond in /usr/local/lib/python3.8/dist-packages (from torch-neuronx) (1.13.1+torchneurond)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement libneuronxla==0.5.669 (from torch-neuronx) (from versions: 1.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for libneuronxla==0.5.669 (from torch-neuronx)\u001b[0m\n",
      "/usr/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import load_entry_point\n",
      "Requirement already up-to-date: torch-xla in /usr/local/lib/python3.8/dist-packages (1.13.1+torchneurond)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from torch-xla) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: cloud-tpu-client>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from torch-xla) (0.10)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/lib/python3/dist-packages (from torch-xla) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: google-api-python-client==1.8.0 in /usr/local/lib/python3.8/dist-packages (from cloud-tpu-client>=0.10.0->torch-xla) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: oauth2client in /usr/local/lib/python3.8/dist-packages (from cloud-tpu-client>=0.10.0->torch-xla) (4.1.3)\n",
      "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/lib/python3/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (0.14.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.26.2)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.34.0)\n",
      "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/lib/python3/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.7 in /usr/lib/python3/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.0.5 in /usr/lib/python3/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla) (4.9)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (5.3.2)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.62.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.19.6)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.31.0)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (3.3.2)\n",
      "Requirement already satisfied, skipping upgrade: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla) (2019.11.28)\n",
      "/usr/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import load_entry_point\n",
      "Requirement already up-to-date: transformers-neuronx in /usr/local/lib/python3.8/dist-packages (0.9.474)\n",
      "Requirement already satisfied, skipping upgrade: accelerate in /usr/local/lib/python3.8/dist-packages (from transformers-neuronx) (0.23.0)\n",
      "Requirement already satisfied, skipping upgrade: safetensors in /usr/local/lib/python3.8/dist-packages (from transformers-neuronx) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: torch-neuronx in /usr/local/lib/python3.8/dist-packages (from transformers-neuronx) (1.13.1.1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: transformers in /usr/local/lib/python3.8/dist-packages (from transformers-neuronx) (4.36.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate->transformers-neuronx) (1.24.4)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate->transformers-neuronx) (23.2)\n",
      "Requirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate->transformers-neuronx) (5.9.8)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /usr/lib/python3/dist-packages (from accelerate->transformers-neuronx) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from accelerate->transformers-neuronx) (1.13.1)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub in /usr/local/lib/python3.8/dist-packages (from accelerate->transformers-neuronx) (0.20.3)\n",
      "Requirement already satisfied, skipping upgrade: torch-xla==1.13.1+torchneurond in /usr/local/lib/python3.8/dist-packages (from torch-neuronx->transformers-neuronx) (1.13.1+torchneurond)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement libneuronxla==0.5.669 (from torch-neuronx->transformers-neuronx) (from versions: 1.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for libneuronxla==0.5.669 (from torch-neuronx->transformers-neuronx)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade optimum-neuron\n",
    "# !pip install --upgrade aws-neuronx-runtime-discovery\n",
    "# !pip install --upgrade libneuronxla\n",
    "# !pip install --upgrade neuronx-cc\n",
    "# !pip install --upgrade neuronx-distributed\n",
    "# !pip install --upgrade neuronx-hwm\n",
    "# !pip install --upgrade torch-neuronx\n",
    "# !pip install --upgrade torch-xla\n",
    "# !pip install --upgrade transformers-neuronx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/pip:6: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import load_entry_point\n",
      "aws-neuronx-runtime-discovery==2.9\n",
      "libneuronxla==1.0\n",
      "neuronx-cc==2.12.68.0+4480452af\n",
      "neuronx-distributed==0.6.0\n",
      "neuronx-hwm==2.12.0.0+422c9037c\n",
      "optimum-neuron==0.0.18\n",
      "tensorboard-plugin-neuronx==2.6.1.0\n",
      "torch-neuronx==1.13.1.1.13.0\n",
      "torch-xla==1.13.1+torchneurond\n",
      "transformers-neuronx==0.9.474\n"
     ]
    }
   ],
   "source": [
    "!pip freeze |grep neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "We will useÂ [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)Â an open source dataset of instruction-following records on categories outlined in theÂ [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "\n",
    "To load theÂ `dolly`Â dataset, we use theÂ `load_dataset()`Â method from the ðŸ¤— Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 6251\n",
      "test dataset size: 1147\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"dreamerdeo/finqa\", split=\"train\")\n",
    "test_dataset = load_dataset(\"dreamerdeo/finqa\", split=\"test\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(f\"test dataset size: {len(test_dataset)}\")\n",
    "# print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_finqa(sample):\n",
    "    post_text = f\"### PostText\\n{sample['post_text']}\"\n",
    "    pre_text = f\"### PreText\\n{sample['pre_text']}\"\n",
    "    question = f\"### Question\\n{sample['question']}\"\n",
    "    answer = f\"### Answer\\n{sample['answer']}\"\n",
    "    gold_evidence = f\"### GoldEvidence\\n{sample['gold_evidence']}\"\n",
    "    table = f\"### Table\\n{sample['table']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [post_text, pre_text, question,answer,gold_evidence, table] if i is not None])\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PostText\n",
      "['as of november 29 , 2013 , the combined amount of accrued interest and penalties related to tax positions taken on our tax returns and included in non-current income taxes payable was approximately $ 11.4 million .', 'we file income tax returns in the u.s .', 'on a federal basis and in many u.s .', 'state and foreign jurisdictions .', 'we are subject to the continual examination of our income tax returns by the irs and other domestic and foreign tax authorities .', 'our major tax jurisdictions are the u.s. , ireland and california .', 'for california , ireland and the u.s. , the earliest fiscal years open for examination are 2005 , 2006 and 2010 , respectively .', 'we regularly assess the likelihood of outcomes resulting from these examinations to determine the adequacy of our provision for income taxes and have reserved for potential adjustments that may result from the current examinations .', 'we believe such estimates to be reasonable ; however , there can be no assurance that the final determination of any of these examinations will not have an adverse effect on our operating results and financial position .', 'in july 2013 , a u.s .', 'income tax examination covering our fiscal years 2008 and 2009 was completed .', 'our accrued tax and interest related to these years was $ 48.4 million and was previously reported in long-term income taxes payable .', 'we settled the tax obligation resulting from this examination with cash and income tax assets totaling $ 41.2 million , and the resulting $ 7.2 million income tax benefit was recorded in the third quarter of fiscal 2013 .', 'the timing of the resolution of income tax examinations is highly uncertain as are the amounts and timing of tax payments that are part of any audit settlement process .', 'these events could cause large fluctuations in the balance sheet classification of current and non-current assets and liabilities .', 'we believe that within the next 12 months , it is reasonably possible that either certain audits will conclude or statutes of limitations on certain income tax examination periods will expire , or both .', 'given the uncertainties described above , we can only determine a range of estimated potential decreases in underlying unrecognized tax benefits ranging from $ 0 to approximately $ 5 million .', 'note 10 .', 'restructuring fiscal 2011 restructuring plan in the fourth quarter of fiscal 2011 , we initiated a restructuring plan consisting of reductions in workforce and the consolidation of facilities in order to better align our resources around our digital media and digital marketing strategies .', 'during fiscal 2013 , we continued to implement restructuring activities under this plan .', 'total costs incurred to date and expected to be incurred for closing redundant facilities are $ 12.2 million as all facilities under this plan have been exited as of november 29 , 2013 .', 'other restructuring plans other restructuring plans include other adobe plans and other plans associated with certain of our acquisitions that are substantially complete .', 'we continue to make cash outlays to settle obligations under these plans , however the current impact to our consolidated financial statements is not significant .', 'our other restructuring plans primarily consist of the 2009 restructuring plan , which was implemented in the fourth quarter of fiscal 2009 , in order to appropriately align our costs in connection with our fiscal 2010 operating plan. .']\n",
      "\n",
      "### PreText\n",
      "['adobe systems incorporated notes to consolidated financial statements ( continued ) accounting for uncertainty in income taxes during fiscal 2013 and 2012 , our aggregate changes in our total gross amount of unrecognized tax benefits are summarized as follows ( in thousands ) : .']\n",
      "\n",
      "### Question\n",
      "for the july 2013 settled examination , what percentage of the cash and income tax assets in the settlement was represented by income tax benefit recorded in the third quarter of fiscal 2013?\n",
      "\n",
      "### Answer\n",
      "17.5%\n",
      "\n",
      "### GoldEvidence\n",
      "['we settled the tax obligation resulting from this examination with cash and income tax assets totaling $ 41.2 million , and the resulting $ 7.2 million income tax benefit was recorded in the third quarter of fiscal 2013 .']\n",
      "\n",
      "### Table\n",
      "[['', '2013', '2012'], ['beginning balance', '$ 160468', '$ 163607'], ['gross increases in unrecognized tax benefits 2013 prior year tax positions', '20244', '1038'], ['gross increases in unrecognized tax benefits 2013 current year tax positions', '16777', '23771'], ['settlements with taxing authorities', '-55851 ( 55851 )', '-1754 ( 1754 )'], ['lapse of statute of limitations', '-4066 ( 4066 )', '-25387 ( 25387 )'], ['foreign exchange gains and losses', '-1474 ( 1474 )', '-807 ( 807 )'], ['ending balance', '$ 136098', '$ 160468']]\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_finqa(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training. This means that we are stacking multiple samples to one sequence and split them with an EOS Token. This makes the training more efficient. Packing/stacking samples can be done during training or before. We will do it before training to save time. We created a utility method [pack_dataset](./scripts/utils/pack_dataset.py) that takes a dataset and a packing function and returns a packed dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Hugging Face model id\n",
    "# model_id = \"codellama/CodeLlama-7b-hf\"\n",
    "# model_id = \"codellama/CodeLlama-13b-hf\"\n",
    "model_id = \"codellama/CodeLlama-34b-hf\"\n",
    "# model_id = \"codellama/CodeLlama-34b-Instruct-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pack/stack our dataset we need to first tokenize it and then we can pack it with the `pack_dataset` method. To prepare our dataset we will now: \n",
    "1. Format our samples using the template method and add an EOS token at the end of each sample\n",
    "2. Tokenize our dataset to convert it from text to tokens\n",
    "3. Pack our dataset to 2048 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PostText\n",
      "['part iii item 10 .', 'directors , executive officers and corporate governance the information required by this item is incorporated by reference to the 201celection of directors 201d section , the 201cdirector selection process 201d section , the 201ccode of conduct 201d section , the 201cprincipal committees of the board of directors 201d section , the 201caudit committee 201d section and the 201csection 16 ( a ) beneficial ownership reporting compliance 201d section of the proxy statement for the annual meeting of stockholders to be held on may 21 , 2015 ( the 201cproxy statement 201d ) , except for the description of our executive officers , which appears in part i of this report on form 10-k under the heading 201cexecutive officers of ipg . 201d new york stock exchange certification in 2014 , our chief executive officer provided the annual ceo certification to the new york stock exchange , as required under section 303a.12 ( a ) of the new york stock exchange listed company manual .', 'item 11 .', 'executive compensation the information required by this item is incorporated by reference to the 201cexecutive compensation 201d section , the 201cnon- management director compensation 201d section , the 201ccompensation discussion and analysis 201d section and the 201ccompensation and leadership talent committee report 201d section of the proxy statement .', 'item 12 .', 'security ownership of certain beneficial owners and management and related stockholder matters the information required by this item is incorporated by reference to the 201coutstanding shares and ownership of common stock 201d section of the proxy statement , except for information regarding the shares of common stock to be issued or which may be issued under our equity compensation plans as of december 31 , 2014 , which is provided in the following table .', 'equity compensation plan information plan category number of shares of common stock to be issued upon exercise of outstanding options , warrants and rights ( a ) 123 weighted-average exercise price of outstanding stock options number of securities remaining available for future issuance under equity compensation plans ( excluding securities reflected in column ( a ) ) equity compensation plans approved by security holders .', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '15563666 9.70 41661517 equity compensation plans not approved by security holders .', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'none 1 included a total of 5866475 performance-based share awards made under the 2009 and 2014 performance incentive plans representing the target number of shares of common stock to be issued to employees following the completion of the 2012-2014 performance period ( the 201c2014 ltip share awards 201d ) , the 2013-2015 performance period ( the 201c2015 ltip share awards 201d ) and the 2014-2016 performance period ( the 201c2016 ltip share awards 201d ) , respectively .', 'the computation of the weighted-average exercise price in column ( b ) of this table does not take the 2014 ltip share awards , the 2015 ltip share awards or the 2016 ltip share awards into account .', '2 included a total of 98877 restricted share units and performance-based awards ( 201cshare unit awards 201d ) which may be settled in shares of common stock or cash .', 'the computation of the weighted-average exercise price in column ( b ) of this table does not take the share unit awards into account .', 'each share unit award actually settled in cash will increase the number of shares of common stock available for issuance shown in column ( c ) .', '3 ipg has issued restricted cash awards ( 201cperformance cash awards 201d ) , half of which shall be settled in shares of common stock and half of which shall be settled in cash .', 'using the 2014 closing stock price of $ 20.77 , the awards which shall be settled in shares of common stock represent rights to an additional 2721405 shares .', 'these shares are not included in the table above .', '4 included ( i ) 29045044 shares of common stock available for issuance under the 2014 performance incentive plan , ( ii ) 12181214 shares of common stock available for issuance under the employee stock purchase plan ( 2006 ) and ( iii ) 435259 shares of common stock available for issuance under the 2009 non-management directors 2019 stock incentive plan. .']\n",
      "\n",
      "### PreText\n",
      "['part iii item 10 .', 'directors , executive officers and corporate governance the information required by this item is incorporated by reference to the 201celection of directors 201d section , the 201cdirector selection process 201d section , the 201ccode of conduct 201d section , the 201cprincipal committees of the board of directors 201d section , the 201caudit committee 201d section and the 201csection 16 ( a ) beneficial ownership reporting compliance 201d section of the proxy statement for the annual meeting of stockholders to be held on may 21 , 2015 ( the 201cproxy statement 201d ) , except for the description of our executive officers , which appears in part i of this report on form 10-k under the heading 201cexecutive officers of ipg . 201d new york stock exchange certification in 2014 , our chief executive officer provided the annual ceo certification to the new york stock exchange , as required under section 303a.12 ( a ) of the new york stock exchange listed company manual .', 'item 11 .', 'executive compensation the information required by this item is incorporated by reference to the 201cexecutive compensation 201d section , the 201cnon- management director compensation 201d section , the 201ccompensation discussion and analysis 201d section and the 201ccompensation and leadership talent committee report 201d section of the proxy statement .', 'item 12 .', 'security ownership of certain beneficial owners and management and related stockholder matters the information required by this item is incorporated by reference to the 201coutstanding shares and ownership of common stock 201d section of the proxy statement , except for information regarding the shares of common stock to be issued or which may be issued under our equity compensation plans as of december 31 , 2014 , which is provided in the following table .', 'equity compensation plan information plan category number of shares of common stock to be issued upon exercise of outstanding options , warrants and rights ( a ) 123 weighted-average exercise price of outstanding stock options number of securities remaining available for future issuance under equity compensation plans ( excluding securities reflected in column ( a ) ) equity compensation plans approved by security holders .', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '15563666 9.70 41661517 equity compensation plans not approved by security holders .', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'none 1 included a total of 5866475 performance-based share awards made under the 2009 and 2014 performance incentive plans representing the target number of shares of common stock to be issued to employees following the completion of the 2012-2014 performance period ( the 201c2014 ltip share awards 201d ) , the 2013-2015 performance period ( the 201c2015 ltip share awards 201d ) and the 2014-2016 performance period ( the 201c2016 ltip share awards 201d ) , respectively .', 'the computation of the weighted-average exercise price in column ( b ) of this table does not take the 2014 ltip share awards , the 2015 ltip share awards or the 2016 ltip share awards into account .', '2 included a total of 98877 restricted share units and performance-based awards ( 201cshare unit awards 201d ) which may be settled in shares of common stock or cash .', 'the computation of the weighted-average exercise price in column ( b ) of this table does not take the share unit awards into account .', 'each share unit award actually settled in cash will increase the number of shares of common stock available for issuance shown in column ( c ) .', '3 ipg has issued restricted cash awards ( 201cperformance cash awards 201d ) , half of which shall be settled in shares of common stock and half of which shall be settled in cash .', 'using the 2014 closing stock price of $ 20.77 , the awards which shall be settled in shares of common stock represent rights to an additional 2721405 shares .', 'these shares are not included in the table above .', '4 included ( i ) 29045044 shares of common stock available for issuance under the 2014 performance incentive plan , ( ii ) 12181214 shares of common stock available for issuance under the employee stock purchase plan ( 2006 ) and ( iii ) 435259 shares of common stock available for issuance under the 2009 non-management directors 2019 stock incentive plan. .']\n",
      "\n",
      "### Question\n",
      "what is the total value of equity compensation plan approved by security holders , ( in millions ) ?\n",
      "\n",
      "### Answer\n",
      "151.0\n",
      "\n",
      "### GoldEvidence\n",
      "['plan category the equity compensation plans approved by security holders of number of shares of common stock to be issued upon exercise of outstanding options warrants and rights ( a ) 123 is 15563666 ; the equity compensation plans approved by security holders of weighted-average exercise price of outstanding stock options ( b ) is 9.70 ; the equity compensation plans approved by security holders of number of securities remaining available for future issuance under equity compensation plans ( excluding securities reflected in column ( a ) ) ( c ) 4 is 41661517 ;']\n",
      "\n",
      "### Table\n",
      "[['plan category', 'number of shares of common stock to be issued upon exercise of outstanding options warrants and rights ( a ) 123', 'weighted-average exercise price of outstanding stock options ( b )', 'number of securities remaining available for future issuance under equity compensation plans ( excluding securities reflected in column ( a ) ) ( c ) 4'], ['equity compensation plans approved by security holders', '15563666', '9.70', '41661517'], ['equity compensation plans not approved by security holders', 'none', '', '']]</s>\n",
      "Chunking dataset into chunks of 2048 tokens.\n",
      "Total number of samples: 4098\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "# add utils method to path for loading dataset\n",
    "import sys\n",
    "sys.path.append(\"./scripts/utils\") # make sure you change this to the correct path \n",
    "from pack_dataset import pack_dataset\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_finqa(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# tokenize dataset\n",
    "dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ")\n",
    "\n",
    "# chunk dataset\n",
    "lm_dataset = pack_dataset(dataset, chunk_length=2048) # We use 2048 as the maximum length for packing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going save it to disk. You could also save it to S3 or the Hugging Face Hub for later use. \n",
    "\n",
    "_Note: Packing and preprocessing your dataset can be run outside of the Trainium instance._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7227f4e0a29496a96a47ddd02ffa000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4098 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save train_dataset to disk\n",
    "dataset_path = \"tokenized_finqa\"\n",
    "lm_dataset.save_to_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf finqa_codellama"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Llama on AWS Trainium using the `NeuronTrainer`\n",
    "\n",
    "Normally you would use theÂ **[Trainer](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.Trainer)**Â andÂ **[TrainingArguments](https://huggingface.co/docs/transformers/v4.19.4/en/main_classes/trainer#transformers.TrainingArguments)**Â to fine-tune PyTorch-based transformer models. \n",
    "\n",
    "But together with AWS, we have developed a `NeuronTrainer` to improve performance, robustness, and safety when training on Trainium instances. The `NeuronTrainer` is part of the `optimum-neuron` library and can be used as a 1-to-1 replacement for the `Trainer`.\n",
    "\n",
    "When it comes to distributed training on AWS Trainium there is a few things we need to take care of. Since Llama is a big model it might not fit on a single accelerator, thats why we added support for different distributed training strategies to the `NeuronTrainer` including: \n",
    "* [ZeRO-1](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/tutorials/training/zero1_gpt2.html): shards the optimizer state over multiple devices.\n",
    "* [Tensor Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/tensor_parallelism_overview.html): shards the model parameters along a given dimension on multiple devices, defined with `tensor_parallel_size`\n",
    "* [Sequence parallelism](https://arxiv.org/pdf/2205.05198.pdf) shards the activations on the sequence axis outside of the tensor parallel regions. It is useful because it saves memory by sharding the activations.\n",
    "* [Pipeline Parallelism](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/pipeline_parallelism_overview.html): _coming soon_\n",
    "\n",
    "\n",
    "We prepared a [run_clm.py](./scripts/run_clm.py), which implements those distributed training strategies for you already. If you want to more about the details you can take a look at the [documentation](https://huggingface.co/docs/optimum-neuron/guides/distributed_training). When training models on AWS Accelerators we first need to compile our model with our training arguments. \n",
    "\n",
    "To overcome this we added a [model cache](https://huggingface.co/docs/optimum-neuron/guides/cache_system), which allows us to use precompiled models and configuration from Hugging Face Hub to skip the compilation step. But every change in the config, will lead to a new compilation, which could result in some cache misses. \n",
    "\n",
    "_Note: If your configuration is not cached please open an issue on [Github](https://github.com/huggingface/optimum-neuron/issues), we are happy to include it._\n",
    "\n",
    "We pre-compiled the config for our training already meaning you can either skip the cell below or rerun it will only take a few minutes since it reuses the cached configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Compiling without a cache can take ~40 minutes. It will also create dummy files in the `dolly_llama_sharded` during compilation you we have to remove them afterwards. We also need to add `MALLOC_ARENA_MAX=64` to limit the CPU allocation to avoid potential crashes, don't remove it for now._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/neuron_parallel_compile\", line 5, in <module>\n",
      "    from libneuronxla.neuron_parallel_compile import main\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/libneuronxla/__init__.py\", line 1, in <module>\n",
      "    raise ImportError(\"WRONG PACKAGE. Please install the package from Neuron Repository - pip.repos.neuron.amazonaws.com\")\n",
      "ImportError: WRONG PACKAGE. Please install the package from Neuron Repository - pip.repos.neuron.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# precompilation command \n",
    "!MALLOC_ARENA_MAX=64 neuron_parallel_compile --command analyze torchrun --nproc_per_node=32 scripts/run_clm.py \\\n",
    " --model_id {model_id} \\\n",
    " --dataset_path {dataset_path} \\\n",
    " --bf16 True \\\n",
    " --learning_rate 5e-5 \\\n",
    " --output_dir finqa_codellama \\\n",
    " --overwrite_output_dir True \\\n",
    " --per_device_train_batch_size 1 \\\n",
    " --gradient_checkpointing True \\\n",
    " --tensor_parallel_size 8 \\\n",
    " --max_steps 10 \\\n",
    " --logging_steps 10 \\\n",
    " --gradient_accumulation_steps 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove dummy artifacts which are created by the precompilation command\n",
    "!rm -rf finqa_codellama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the compilation is done we can start our training with a similar command, we just need to remove the `neuron_parallel_compile`. We will use `torchrun` to launch our training script. `torchrun` is a tool that automatically distributes a PyTorch model across multiple accelerators. We can pass the number of accelerators as `nproc_per_node` arguments alongside our hyperparameters. \n",
    "The difference to the compilation command is that we changed from `max_steps=10` to `num_train_epochs=3`.\n",
    "\n",
    "Launch the training, with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!MALLOC_ARENA_MAX=64 torchrun --nproc_per_node=32 scripts/run_clm.py \\\n",
    " --model_id {model_id} \\\n",
    " --dataset_path {dataset_path} \\\n",
    " --bf16 True \\\n",
    " --learning_rate 5e-5 \\\n",
    " --output_dir finqa_codellama \\\n",
    " --overwrite_output_dir True \\\n",
    " --skip_cache_push True \\\n",
    " --per_device_train_batch_size 1 \\\n",
    " --gradient_checkpointing True \\\n",
    " --tensor_parallel_size 8 \\\n",
    " --num_train_epochs 3 \\\n",
    " --logging_steps 10 \\\n",
    " --gradient_accumulation_steps 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it, we successfully trained Llama 7B on AWS Trainium. The training took for 3 epochs on dolly (15k samples) took 43:24 minutes where the raw training time was only 31:46 minutes. This leads to a cost of ~$15.5 for the e2e training on the trn1.32xlarge instance. Not Bad! \n",
    "\n",
    "But before we can share and test our model we need to consolidate our model. Since we used Tensor Parallelism during training, we need to consolidate the model weights before we can use it. Tensor Parallelism shards the model weights accross different workers, only sharded checkpoints will be saved during training.\n",
    "\n",
    "The Optimum CLI provides a way of doing that very easily via the `optimum neuron consolidate`` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!optimum-cli neuron consolidate finqa_codellama/tensor_parallel_shards finqa_codellama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets remove our \"sharded\" checkpoints as we have consolidated them already to safetensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf finqa_codellama/tensor_parallel_shards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evalaute and test fine-tuned Llama model\n",
    "\n",
    "Similar to training to be able to run inferece on AWS Trainium or AWS Inferentia2 we need to compile our model for the correct use. We will use our Trainium instance for the inference test, but we recommend customer to switch to Inferentia2 for inference. \n",
    "\n",
    "Optimum Neuron implements similar to Transformers AutoModel classes for easy inference use. We will use  the `NeuronModelForCausalLM` class to load our vanilla transformers checkpoint and convert it to neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.neuron import NeuronModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "compiler_args = {\"num_cores\": 2, \"auto_cast_type\": 'fp16'}\n",
    "input_shapes = {\"batch_size\": 1, \"sequence_length\": 2048}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"finqa_codellama\")\n",
    "model = NeuronModelForCausalLM.from_pretrained(\n",
    "        \"finqa_codellama\",\n",
    "        export=True,\n",
    "        **compiler_args,\n",
    "        **input_shapes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Inference compilation can take ~25minutes. Luckily, you need to only run this onces. Since you can save the model afterwards. If you are going to run on Inferentia2 you need to recompile again. The compilation is parameter and hardware specific._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENT IN if you want to save the compiled model\n",
    "# model.save_pretrained(\"compiled_finqa_codellama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test inference, but have to make sure we format our input to our prompt format we used for fine-tuning. Therefore we created a helper method, which accepts a `dict` with our `instruction` and optionally a `context`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_finqa_infernece(sample):\n",
    "    post_text = f\"### PostText\\n{sample['post_text']}\"\n",
    "    pre_text = f\"### PreText\\n{sample['pre_text']}\"\n",
    "    table = f\"### Table\\n{sample['table']}\"    \n",
    "    question = f\"### Question\\n{sample['question']}\"    \n",
    "    answer = f\"### Answer\\n\"\n",
    "    gold_evidence = f\"### GoldEvidence\\n\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [pre_text,post_text,table, question,answer,gold_evidence] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate(sample): \n",
    "    prompt = format_finqa_infernece(sample)\n",
    "    print(\"prompt\",prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs,\n",
    "                         max_new_tokens=512,\n",
    "                         do_sample=True,\n",
    "                         temperature=0.9,\n",
    "                         top_k=50,\n",
    "                         top_p=0.9)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=False)[len(prompt):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test inference. First we test without a context.\n",
    "\n",
    "_Note: Inference is not expected to be super fast on AWS Trainium using 2 cores. For Inference we recommend using Inferentia2._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_element = test_dataset[randrange(len(test_dataset))]\n",
    "prompt = {\n",
    "  \"pre_text\": test_data_element['pre_text'],\n",
    "  \"post_text\": test_data_element['post_text'],\n",
    "  \"question\": test_data_element['question'],\n",
    "  \"table\": test_data_element['table'],\n",
    "    \"gold_evidence\": test_data_element['gold_evidence']\n",
    "}\n",
    "res = generate(prompt)\n",
    "print(\"================================\")\n",
    "print(\"Actual Answer\",res)\n",
    "print(\"Expected Answer\",test_data_element['answer'])\n",
    "print(\"================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AWS stands for Amazon Web Services. AWS is a suite of remote computing services offered by Amazon. The most widely used of these include Amazon Elastic Compute Cloud (Amazon EC2), which provides resizable compute capacity in the cloud; Amazon Simple Storage Service (Amazon S3), which is an object storage service; and Amazon Elastic Block Store (Amazon EBS), which is designed to provide high performance, durable block storage volumes for use with AWS instances. AWS also provides other services, such as AWS Identity and Access Management (IAM), a service that enables organizations to control access to their AWS resources, and AWS Key Management Service (AWS KMS), which helps customers create and control the use of encryption keys.</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks correct. Now, lets add some context, e.g. as you would do for RAG applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, our model also correctly uses the provided context. We are done. Congrats on fine-tuning Code Llama on AWS Trainium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
